

```markdown
# ğŸ“š Adaptive AI Storytelling Dashboard

This repository contains the code for an interactive dashboard that compares different storytelling strategies using the **Mistral-7B** language model. The interface allows users to test two experiments:

- **Experiment 1**: RL-tuned vs static model comparison
- **Experiment 2**: Structured vs open-ended prompt generation

Built using **Streamlit**, the dashboard enables real-time story generation, user feedback collection, and BLEU score visualization.

---

## ğŸ“ Project Structure

```

ğŸ“¦streamlit\_dashboard/
â”£ ğŸ“œ dashboard.py             # Main app entry point (Streamlit routing logic)
â”£ ğŸ“ pages/
â”ƒ â”£ ğŸ“œ experiment1.py         # A/B comparison between RL-tuned and static models
â”ƒ â”— ğŸ“œ experiment2.py         # Prompt style experiment (structured vs open-ended)
â”£ ğŸ“ prompts/
â”ƒ â”— ğŸ“œ test\_prompts.json      # Curated test prompts for both experiments
â”£ ğŸ“ feedback\_data/           # Directory where JSON feedback is saved (autogenerated)
â”£ ğŸ“ logs/                    # Reward curve logs (if PPO simulation is run)
â”£ ğŸ“ model/                   # âš ï¸ Not included â€” contains model weights (see note below)

````

---

## ğŸ§  Requirements

To run this dashboard locally, you will need:

- **Python 3.9+**
- **Streamlit** (`pip install streamlit`)
- **NLTK**, **NumPy**, **Pandas**, **Matplotlib**, **Scikit-learn**
- **Llama.cpp** or **mlx-lm** backend (for local Mistral model loading, optional)

> âš ï¸ **Note**: Due to file size restrictions, the actual 4-bit quantized model files (e.g., `.gguf` for Mistral-7B) are **not included** in this repository. You must download and place them in the `model/` directory if running locally. You can find quantized models at [Mistral.ai](https://mistral.ai) or use `ggml`/`mlx-lm`.

---

## ğŸš€ How to Run

1. Clone the repo and install dependencies:
   ```bash
   pip install -r requirements.txt
````

2. Start the Streamlit app:

   ```bash
   streamlit run dashboard.py
   ```

3. Use the sidebar to navigate between:

   * ğŸ“Š `experiment1.py`: Compare RL-tuned vs static model stories
   * âœï¸ `experiment2.py`: Generate and rate structured/open-ended stories

---

## ğŸ“ Feedback Storage

* User ratings are stored in `/feedback_data/` as `.json` files
* Ratings cover:

  * Coherence
  * Creativity
  * Engagement
  * Twist (optional)

These ratings are used for simulated reinforcement learning.

---

## ğŸ”’ Privacy & Security

No user-identifiable data is collected. Data is stored locally for academic analysis only.

---

## ğŸ“¦ Missing Files Disclaimer

To keep the repository lightweight, the following were **not uploaded**:

* `*.gguf` model weights for Mistral-7B
* BLEU reference files
* Any long video walkthroughs or training datasets

If needed, please contact the author or reconfigure paths to your own local model files.

---

Developed by **Phebe Bonuah Ameyaw**
University of Nottingham Â· 2025

```

Let me know if you want it adapted for a coursework submission instead of public GitHub use.
```
