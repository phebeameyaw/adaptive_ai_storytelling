

```markdown
# 📚 Adaptive AI Storytelling Dashboard

This repository contains the code for an interactive dashboard that compares different storytelling strategies using the **Mistral-7B** language model. The interface allows users to test two experiments:

- **Experiment 1**: RL-tuned vs static model comparison
- **Experiment 2**: Structured vs open-ended prompt generation

Built using **Streamlit**, the dashboard enables real-time story generation, user feedback collection, and BLEU score visualization.

---

## 📁 Project Structure

```

📦streamlit\_dashboard/
┣ 📜 dashboard.py             # Main app entry point (Streamlit routing logic)
┣ 📁 pages/
┃ ┣ 📜 experiment1.py         # A/B comparison between RL-tuned and static models
┃ ┗ 📜 experiment2.py         # Prompt style experiment (structured vs open-ended)
┣ 📁 prompts/
┃ ┗ 📜 test\_prompts.json      # Curated test prompts for both experiments
┣ 📁 feedback\_data/           # Directory where JSON feedback is saved (autogenerated)
┣ 📁 logs/                    # Reward curve logs (if PPO simulation is run)
┣ 📁 model/                   # ⚠️ Not included — contains model weights (see note below)

````

---

## 🧠 Requirements

To run this dashboard locally, you will need:

- **Python 3.9+**
- **Streamlit** (`pip install streamlit`)
- **NLTK**, **NumPy**, **Pandas**, **Matplotlib**, **Scikit-learn**
- **Llama.cpp** or **mlx-lm** backend (for local Mistral model loading, optional)

> ⚠️ **Note**: Due to file size restrictions, the actual 4-bit quantized model files (e.g., `.gguf` for Mistral-7B) are **not included** in this repository. You must download and place them in the `model/` directory if running locally. You can find quantized models at [Mistral.ai](https://mistral.ai) or use `ggml`/`mlx-lm`.

---

## 🚀 How to Run

1. Clone the repo and install dependencies:
   ```bash
   pip install -r requirements.txt
````

2. Start the Streamlit app:

   ```bash
   streamlit run dashboard.py
   ```

3. Use the sidebar to navigate between:

   * 📊 `experiment1.py`: Compare RL-tuned vs static model stories
   * ✍️ `experiment2.py`: Generate and rate structured/open-ended stories

---

## 📝 Feedback Storage

* User ratings are stored in `/feedback_data/` as `.json` files
* Ratings cover:

  * Coherence
  * Creativity
  * Engagement
  * Twist (optional)

These ratings are used for simulated reinforcement learning.

---

## 🔒 Privacy & Security

No user-identifiable data is collected. Data is stored locally for academic analysis only.

---

## 📦 Missing Files Disclaimer

To keep the repository lightweight, the following were **not uploaded**:

* `*.gguf` model weights for Mistral-7B
* BLEU reference files
* Any long video walkthroughs or training datasets

If needed, please contact the author or reconfigure paths to your own local model files.

---

Developed by **Phebe Bonuah Ameyaw**
University of Nottingham · 2025

```

Let me know if you want it adapted for a coursework submission instead of public GitHub use.
```
